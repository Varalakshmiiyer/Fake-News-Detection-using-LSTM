{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "df = pd.read_csv(\"fake_or_real_news.csv\")\n",
    "    \n",
    "\n",
    "\n",
    "df.title = df.title.str.lower()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df= df.drop(df.columns[df.columns.str.contains('unnamed',case = False)],axis = 1)\n",
    "\n",
    "df['title']=df['title'].str.lower()\n",
    "\n",
    "y = df.label \n",
    "\n",
    "\n",
    "df.drop(\"label\", axis=1)   \n",
    "\n",
    "\n",
    "\n",
    "df.title = df.title.str.replace(r'http[\\w:/\\.]+','<URL>') \n",
    "\n",
    "df.title = df.title.str.replace(r'[^\\.\\w\\s]','') \n",
    "df.title = df.title.str.replace(r'\\.\\.+','.') \n",
    "\n",
    "df.title = df.title.str.replace(r'\\.',' . ') \n",
    "\n",
    "df.title = df.title.str.replace(r'\\s\\s+',' ') \n",
    "\n",
    "df.title = df.title.str.strip() \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['you can smell hillarys fear',\n",
       "       'watch the exact moment paul ryan committed political suicide at a trump rally video',\n",
       "       'kerry to go to paris in gesture of sympathy', ...,\n",
       "       'antitrump protesters are tools of the oligarchy information',\n",
       "       'in ethiopia obama seeks progress on peace security in east africa',\n",
       "       'jeb bush is suddenly attacking trump . heres why that matters'], dtype=object)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.title.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "all_text = ' '.join(df.title.values)\n",
    "words = all_text.split()\n",
    "u_words = Counter(words).most_common()\n",
    "u_words_counter = u_words\n",
    "u_words_frequent = [word[0] for word in u_words if word[1]>5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, BatchNormalization, LSTM, Embedding, Reshape\n",
    "from keras.models import load_model, model_from_json\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "from os import mkdir, makedirs, remove, listdir\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "with open('./dataglove/glove.6B.50d.txt','rb') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "glove_weights = np.zeros((len(lines), 50))\n",
    "words = []\n",
    "for i, line in enumerate(lines):\n",
    "    word_weights = line.split()\n",
    "    words.append(word_weights[0])\n",
    "    weight = word_weights[1:]\n",
    "    glove_weights[i] = np.array([float(w) for w in weight])\n",
    "word_vocab = [w.decode(\"utf-8\") for w in words]\n",
    "\n",
    "word2glove = dict(zip(word_vocab, glove_weights))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "u_words_total = [k for k,v in u_words_counter]\n",
    "word_vocab = dict(zip(word_vocab, range(len(word_vocab))))\n",
    "word_in_glove = np.array([w in word_vocab for w in u_words_total])\n",
    "\n",
    "words_in_glove = [w for w,is_true in zip(u_words_total,word_in_glove) if is_true]\n",
    "words_not_in_glove = [w for w,is_true in zip(u_words_total,word_in_glove) if not is_true]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2num = dict(zip(words_in_glove,range(len(words_in_glove))))\n",
    "len_glove_words = len(word2num)\n",
    "freq_words_not_glove = [w for w in words_not_in_glove if w in u_words_frequent]\n",
    "b = dict(zip(freq_words_not_glove,range(len(word2num), len(word2num)+len(freq_words_not_glove))))\n",
    "word2num = dict(**word2num, **b)\n",
    "word2num['<Other>'] = len(word2num)\n",
    "num2word = dict(zip(word2num.values(), word2num.keys()))\n",
    "df_new = df[df['title'].notnull()]\n",
    "int_text = [[word2num[word] if word in word2num else word2num['<Other>'] \n",
    "             for word in content.split()]  for content in df.title.values\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "num2word[len(word2num)] = '<PAD>'\n",
    "word2num['<PAD>'] = len(word2num)\n",
    "\n",
    "for i, t in enumerate(int_text):\n",
    "    if len(t)<500:\n",
    "        int_text[i] = [word2num['<PAD>']]*(500-len(t)) + t\n",
    "    elif len(t)>500:\n",
    "        int_text[i] = t[:500]\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "x = np.array(int_text)\n",
    "y = (df.label.values=='REAL').astype('int')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import logistic\n",
    "X_train=logistic.cdf(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824,\n",
       "        9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824,\n",
       "        9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824,\n",
       "        9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824,\n",
       "        9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824,\n",
       "        9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824,\n",
       "        9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824,\n",
       "        9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824,\n",
       "        9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824,\n",
       "        9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824,\n",
       "        9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824,\n",
       "        9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824,\n",
       "        9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824,\n",
       "        9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824,\n",
       "        9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824,\n",
       "        9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824,\n",
       "        9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824,\n",
       "        9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824,\n",
       "        9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824,\n",
       "        9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824,\n",
       "        9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824,\n",
       "        9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824,\n",
       "        9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824,\n",
       "        9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824,\n",
       "        9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824,\n",
       "        9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824,\n",
       "        9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824,\n",
       "        9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824,\n",
       "        9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824,\n",
       "        9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824,\n",
       "        9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824,\n",
       "        9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824,\n",
       "        9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824,\n",
       "        9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824,\n",
       "        9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824,\n",
       "        9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824,\n",
       "        9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824,\n",
       "        9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824,\n",
       "        9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824,\n",
       "        9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824,\n",
       "        9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824,\n",
       "        9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824,\n",
       "        9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824,\n",
       "        9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824, 9824,\n",
       "        9824, 9824,  152,    0, 5035,  551,  100,  200, 2569,  107,  878,\n",
       "          20,    8,    4,  254,   33]])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x[1:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  0.99330715,  1.        ]])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(\"../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "data=pd.read_csv(\"../input/train.csv\")\n",
    "textdata=data['question_text'].str.lower()\n",
    "labels=data['target']\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "MAX_NB_WORDS=200000\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(textdata)\n",
    "sequences = tokenizer.texts_to_sequences(textdata)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "x_traindata = pad_sequences(sequences, maxlen=200)\n",
    "\n",
    "y_labels = to_categorical(np.asarray(labels))\n",
    "model=word2vec\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "\tembedding_vector = model.wv[model.wv.index2word[i]]\n",
    "\tif embedding_vector is not None:\n",
    "\t\tembedding_matrix[i] = embedding_vector\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 50)          491250    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                29440     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 520,755\n",
      "Trainable params: 520,755\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word2num), 50)) # , batch_size=batch_size\n",
    "model.add(LSTM(64)\n",
    "         )\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5701 samples, validate on 634 samples\n",
      "Epoch 1/5\n",
      "5701/5701 [==============================] - 33s 6ms/step - loss: 0.7128 - acc: 0.5031 - val_loss: 0.4884 - val_acc: 0.7823\n",
      "Epoch 2/5\n",
      "5701/5701 [==============================] - 39s 7ms/step - loss: 0.6956 - acc: 0.4936 - val_loss: 0.5073 - val_acc: 0.7744\n",
      "Epoch 3/5\n",
      "5701/5701 [==============================] - 37s 7ms/step - loss: 0.6937 - acc: 0.5039 - val_loss: 0.5286 - val_acc: 0.7461\n",
      "Epoch 4/5\n",
      "5701/5701 [==============================] - 39s 7ms/step - loss: 0.6933 - acc: 0.4971 - val_loss: 0.5515 - val_acc: 0.7224\n",
      "Epoch 5/5\n",
      "5701/5701 [==============================] - 40s 7ms/step - loss: 0.6927 - acc: 0.5094 - val_loss: 0.5655 - val_acc: 0.7145\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x241453b9518>"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 50\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=100, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.51062495]], dtype=float32)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"8 classic football banners of our time\".lower()\n",
    "sentence_num = [word2num[w] if w in word2num else word2num['<Other>'] for w in sentence.split()]\n",
    "sentence_num = [word2num['<PAD>']]*(500-len(sentence_num)) + sentence_num\n",
    "\n",
    "sentence_num = np.array(sentence_num)\n",
    "sentence_num=logistic.cdf(sentence_num)\n",
    "model.predict(sentence_num[None,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "634/634 [==============================] - 1s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.56549168197138455, 0.71451104194960002]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
